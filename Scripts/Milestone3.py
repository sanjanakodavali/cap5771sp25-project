# -*- coding: utf-8 -*-
"""Milestone3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CmMWJnrFmt6P2jRHTdp78GabnO6LrF7u
"""

# For New Data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
import json

from google.colab import drive
drive.mount('/content/drive')

dataset_paths = [
    '/content/drive/MyDrive/Data/medicine_prescription_records.csv',
    '/content/drive/MyDrive/Data/medicine_dataset.csv',
    '/content/drive/MyDrive/Data/medicine_data.csv',
    '/content/drive/MyDrive/Data/drugsComTrain_raw.csv',
    '/content/drive/MyDrive/Data/drug200.csv'
]

def display_csv_heads(paths):
    for path in paths:
        print(f"\n File: {path}")
        try:
            df = pd.read_csv(path)
            print(df.head())
            print(f" Shape: {df.shape} | Columns: {list(df.columns)}\n")
            print("-" * 80)
        except Exception as e:
            print(f" Could not read {path}. Error: {e}\n")

display_csv_heads(dataset_paths)

#  Data Preprocessing

print("Loading datasets...")
medicine_prescription = pd.read_csv('/content/drive/MyDrive/Data/medicine_prescription_records.csv')
medicine_dataset = pd.read_csv('/content/drive/MyDrive/Data/medicine_dataset.csv', low_memory=False)
medicine_data = pd.read_csv('/content/drive/MyDrive/Data/medicine_data.csv')
drugs_reviews = pd.read_csv('/content/drive/MyDrive/Data/drugsComTrain_raw.csv')
drug200 = pd.read_csv('/content/drive/MyDrive/Data/drug200.csv')
print("Datasets loaded successfully.\n")

print("Starting data preprocessing...\n")

# 1. Handling Missing Values
print("Handling missing values...")
medicine_prescription.dropna(inplace=True)
medicine_dataset.fillna("Unknown", inplace=True)
medicine_data.dropna(inplace=True)
drugs_reviews.dropna(subset=['condition', 'review'], inplace=True)
drug200.dropna(inplace=True)
print("Missing values handled.\n")

# 2. Dealing with Mixed Data Types
print("Converting product_price to numeric format...")
medicine_dataset = medicine_dataset.convert_dtypes()
medicine_data['product_price'] = pd.to_numeric(
    medicine_data['product_price'].replace('[\\u20B9,]', '', regex=True), errors='coerce'
)
medicine_data.dropna(subset=['product_price'], inplace=True)
print("Data types converted.\n")

# 3. Parsing drug_interactions JSON column (with enhanced error handling)
print("Parsing drug_interactions column...")
def parse_json_column(x):
    try:
        if isinstance(x, str):
            return list(json.loads(x).get('drug', []))
        return []
    except (json.JSONDecodeError, TypeError, ValueError):
        return []

medicine_data['drug_interactions'] = medicine_data['drug_interactions'].apply(parse_json_column)
print("JSON parsing complete.\n")

# 4. Feature Engineering: Sentiment Analysis on reviews
print("Performing sentiment analysis on reviews...")
drugs_reviews['sentiment_score'] = drugs_reviews['review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)
print("Sentiment scores calculated.\n")

# 5. Data Integration: Merging datasets
print("Merging datasets...")
merged_data = pd.merge(drugs_reviews, medicine_dataset, left_on='drugName', right_on='name', how='left')
merged_data = pd.merge(merged_data, medicine_data, left_on='drugName', right_on='product_name', how='left')
print(f"Merged dataset shape: {merged_data.shape}\n")

# Save preprocessed data
merged_data.to_csv('/content/preprocessed_merged_data.csv', index=False)
print("Preprocessed data saved successfully.\n")

# Exploratory Data Analysis (EDA)

print("Starting Enhanced EDA for Project Objective...\n")

# 1. Summary Statistics
print("Descriptive Statistics for Ratings:\n", merged_data['rating'].describe())
print("Descriptive Statistics for Product Prices:\n", merged_data['product_price'].describe())

# 2. Rating Distribution
plt.figure(figsize=(10, 6))
sns.histplot(merged_data['rating'], bins=10, kde=True, color='skyblue')
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# 3. Sentiment Score Distribution
plt.figure(figsize=(10, 6))
sns.histplot(merged_data['sentiment_score'], bins=20, kde=True, color='lightgreen')
plt.title('Sentiment Score Distribution in Reviews')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

# 4. Top Conditions by Rating
top_conditions = merged_data.groupby('condition')['rating'].mean().sort_values(ascending=False).head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_conditions.index, y=top_conditions.values, palette='viridis')
plt.title('Top 10 Conditions by Average Rating')
plt.ylabel('Average Rating')
plt.xticks(rotation=45)
plt.show()

# 7. Sentiment vs Rating
plt.figure(figsize=(10, 6))
sns.scatterplot(x='sentiment_score', y='rating', data=merged_data, alpha=0.5)
plt.title('Sentiment Score vs. Rating')
plt.xlabel('Sentiment Score')
plt.ylabel('Rating')
plt.show()

# 8. Prescription Trends by Specialty
top_specialties = medicine_prescription['specialty'].value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_specialties.index, y=top_specialties.values, palette='muted')
plt.title('Top 10 Specialties by Prescription Count')
plt.ylabel('Prescriptions')
plt.xticks(rotation=45)
plt.show()

# 9. Outlier Detection for Key Features
fig, axes = plt.subplots(1, 2, figsize=(16, 6))
sns.boxplot(y=merged_data['rating'].dropna(), ax=axes[0], color='coral').set(title='Rating Outliers')
sns.boxplot(y=merged_data['product_price'].dropna(), ax=axes[1], color='lightblue').set(title='Product Price Outliers')
plt.show()

# 10. Drug Interaction Frequency
drug_interactions_flat = [item for sublist in medicine_data['drug_interactions'] for item in sublist]
interaction_counts = pd.Series(drug_interactions_flat).value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=interaction_counts.index, y=interaction_counts.values, palette='Set2')
plt.title('Top 10 Common Drug Interactions')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

# 11. Patient Demographics Analysis
drug200['BP'] = drug200['BP'].astype(str)
sns.countplot(x='BP', data=drug200, palette='pastel')
plt.title('Blood Pressure Distribution in Patients')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(drug200['Na_to_K'], bins=30, kde=True)
plt.title('Distribution of Na_to_K Ratio')
plt.xlabel('Na_to_K Ratio')
plt.ylabel('Frequency')
plt.show()

# --- Milestone 2: Feature Engineering ---

from sklearn.preprocessing import LabelEncoder
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, f1_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler

# Calculate length of each review
merged_data['review_length'] = merged_data['review'].astype(str).apply(len)

# Log transformation to normalize skew in 'usefulCount'
merged_data['log_usefulCount'] = np.log1p(merged_data['usefulCount'])

# Create an interaction term between sentiment and usefulness
merged_data['sentiment_useful_interaction'] = merged_data['sentiment_score'] * merged_data['log_usefulCount']

# Average rating for each condition - might help the model learn better patterns
avg_rating = merged_data.groupby('condition')['rating'].mean().to_dict()
merged_data['avg_rating_per_condition'] = merged_data['condition'].map(avg_rating)

# Encode categorical fields like condition and manufacturer
from sklearn.preprocessing import LabelEncoder

label_enc = LabelEncoder()
merged_data['condition_encoded'] = label_enc.fit_transform(merged_data['condition'].astype(str))
merged_data['manufacturer_encoded'] = label_enc.fit_transform(merged_data['product_manufactured'].astype(str))

# Define binary classification target (1 = good rating, 0 = low rating)
merged_data['good_rating'] = merged_data['rating'].apply(lambda x: 1 if x >= 7 else 0)

# --- Feature Selection ---
# Looking at how features relate to each other

import matplotlib.pyplot as plt
import seaborn as sns

# Select important features for correlation
selected_features = [
    'sentiment_score', 'review_length', 'log_usefulCount',
    'avg_rating_per_condition', 'sentiment_useful_interaction',
    'condition_encoded', 'manufacturer_encoded'
]

# Calculate correlation between features
corr = merged_data[selected_features].corr()

# Plot heatmap with better spacing and visibility
plt.figure(figsize=(14, 10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, square=True,
            cbar_kws={"shrink": 0.75})

plt.xticks(rotation=35, ha='right')
plt.yticks(rotation=0)
plt.title("Correlation Matrix", fontsize=16)
plt.tight_layout(pad=2.0)
plt.show()

# --- Train-Test Split + Modeling ---

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler

# Prepare feature matrix and target
X = merged_data[selected_features].dropna()
y = merged_data.loc[X.index, 'good_rating']

# Split into training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize numeric values for models that need it
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Model Training and Evaluation ---

# Set up models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(probability=True)
}

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

all_metrics = []

# Train and test each model
for name, model in models.items():
    print(f"\n== {name} ==")

    # Train
    if name == 'Random Forest':
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        probs = model.predict_proba(X_test)[:, 1]
    else:
        model.fit(X_train_scaled, y_train)
        preds = model.predict(X_test_scaled)
        probs = model.predict_proba(X_test_scaled)[:, 1]

    # Evaluation
    print(classification_report(y_test, preds))
    auc_score = roc_auc_score(y_test, probs)

    all_metrics.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, preds),
        'Precision': precision_score(y_test, preds),
        'Recall': recall_score(y_test, preds),
        'F1-Score': f1_score(y_test, preds),
        'ROC-AUC': auc_score
    })

    # Plot ROC
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.2f})')

# ROC Curve
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend()
plt.show()
# Collect model metrics
metrics_list = []

for name, model in models.items():
    if name == 'Random Forest':
        preds = model.predict(X_test)
        probs = model.predict_proba(X_test)[:, 1]
    else:
        preds = model.predict(X_test_scaled)
        probs = model.predict_proba(X_test_scaled)[:, 1]

    metrics_list.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, preds),
        'Precision': precision_score(y_test, preds),
        'Recall': recall_score(y_test, preds),
        'F1-Score': f1_score(y_test, preds),
        'ROC-AUC': roc_auc_score(y_test, probs)
    })

# Create DataFrame
metrics_df = pd.DataFrame(metrics_list)

# Save metrics for Streamlit
metrics_df.to_csv('model_metrics.csv', index=False)
print("Saved model_metrics.csv")

# Extract feature importances from Random Forest
rf_model = models['Random Forest']

feat_imp = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Save feature importances for Streamlit
feat_imp.to_csv('feature_importances.csv', index=False)
print("Saved feature_importances.csv")

# Save ROC curve image
plt.figure()
for name, model in models.items():
    if name == 'Random Forest':
        probs = model.predict_proba(X_test)[:, 1]
    else:
        probs = model.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=f'{name}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.tight_layout()
plt.savefig('roc_curve.png')
plt.close()
print("Saved roc_curve.png")

# Grab the trained Random Forest model from your dictionary
rf_model = models['Random Forest']

# Extract and save feature importances
import pandas as pd

feat_imp = pd.DataFrame({
    'Feature': X_train.columns,  # or X.columns if applicable
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Save to CSV
feat_imp.to_csv('feature_importances.csv', index=False)
print("Saved feature_importances.csv")

# Save ROC curve image
plt.figure()
for name, model in models.items():
    if name == 'Random Forest':
        probs = model.predict_proba(X_test)[:, 1]
    else:
        probs = model.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=f'{name}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.tight_layout()
plt.savefig('roc_curve.png')
plt.close()
print("Saved roc_curve.png")

# --- Compare Models in a Table ---

results_df = pd.DataFrame(all_metrics).sort_values(by='F1-Score', ascending=False)
print(results_df)

# Optional: Plot bar chart comparison
results_df.set_index('Model')[['F1-Score', 'ROC-AUC']].plot(kind='bar', figsize=(10, 6), colormap='Set2')
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# --- Compare Models in a Table ---

results_df = pd.DataFrame(all_metrics).sort_values(by='F1-Score', ascending=False)
print(results_df)

# Optional: Plot bar chart comparison
results_df.set_index('Model')[['F1-Score', 'ROC-AUC']].plot(kind='bar', figsize=(10, 6), colormap='Set2')
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Milestone - 3

# Evaluation + Interpretation Code

# Import necessary libraries
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.ensemble import RandomForestClassifier
import shap
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Train the model (Random Forest)
best_model = RandomForestClassifier(random_state=42)
best_model.fit(X_train, y_train)

# Predict
y_pred = best_model.predict(X_test)
y_probs = best_model.predict_proba(X_test)[:, 1]

# Classification report
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

# --- SHAP Interpretation (Crash-Safe) ---

# 1. Take a small sample to limit memory
X_sample = X_test.sample(n=20, random_state=42)

# 2. Rebuild as DataFrame to ensure correct columns
X_sample = pd.DataFrame(X_sample.values, columns=X_train.columns)

# 3. Compute SHAP values
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_sample)   # shape = (20, 7, 2)

# 4. Extract class-1 values: (n_samples, n_features)
shap_class1 = shap_values[:, :, 1]              # now (20, 7)

# 5. Compute mean absolute SHAP per feature
mean_abs_shap = np.abs(shap_class1).mean(axis=0)

# 6. Build a DataFrame and sort by importance
shap_df = pd.DataFrame({
    'Feature': X_sample.columns,
    'Mean |SHAP|': mean_abs_shap
}).sort_values(by='Mean |SHAP|', ascending=False)

# 7. Plot with plain matplotlib
plt.figure(figsize=(8, 5))
plt.barh(shap_df['Feature'], shap_df['Mean |SHAP|'])
plt.gca().invert_yaxis()
plt.xlabel("Mean |SHAP Value|")
plt.title("Feature Importance (SHAP for Class 1)")
plt.tight_layout()
plt.show()

# after your plotting code
plt.tight_layout()
plt.savefig("shap_class1_bar.png", dpi=300, bbox_inches="tight")
print("Saved shap_class1_bar.png")

shap_df.to_csv("shap_class1_bar.csv", index=False)
print("Saved shap_class1_bar.csv")

# shap.summary_plot(shap_class1, X_sample)

# 1. Create a new figure (optionally set size)
plt.figure(figsize=(8, 6))

# 2. Draw the summary plot without showing it
shap.summary_plot(shap_class1, X_sample, show=False)

# 3. Grab the current figure and save it
plt.gcf().savefig("shap_summary_beeswarm.png", bbox_inches="tight")

# 4. Close to free memory
plt.close()

import os
import matplotlib
matplotlib.use("Agg")      # Use a non-interactive backend
import matplotlib.pyplot as plt
import shap

# Disable any GPU ops
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Sample index
i = 0

# Generate the force plot purely on CPU/back-end
fig = shap.force_plot(
    explainer.expected_value[1],
    shap_class1[i, :],
    X_sample.iloc[i, :],
    matplotlib=True,
    show=False
 )

# Save to file
fig.savefig("shap_force_plot_sample_0.png", bbox_inches="tight")
plt.close(fig)

print("Force plot saved as shap_force_plot_sample_0.png")

# 1. Make sure joblib is imported
import joblib

# 2. Dump your trained model to a file
joblib.dump(best_model, "best_rf_model.joblib")
print("âœ… Saved Random Forest to best_rf_model.joblib")

import pandas as pd

# --- assuming you already have your splits X_train, X_test, y_train, y_test ---

# 1. Save X_test (keep the index so read_csv(index_col=0) works)
X_test.to_csv("X_test.csv", index=True)

# 2. Convert y_test (array or Series) into a Series with the same index, then save
y_series = pd.Series(y_test, index=X_test.index, name="target")
y_series.to_csv("y_test.csv", index=True)

print("âœ… Wrote X_test.csv and y_test.csv to disk")

"""# Streamlit"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > app.py << 'EOF'
# import os
# # 0) force CPUâ€only for SHAP force_plot
# os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
# import matplotlib
# matplotlib.use("Agg")
# 
# import streamlit as st
# import streamlit.components.v1 as components
# import pandas as pd
# import numpy as np
# import joblib
# import shap
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.metrics import (
#     classification_report,
#     confusion_matrix,
#     ConfusionMatrixDisplay,
#     precision_recall_curve,
#     RocCurveDisplay
# )
# from streamlit_lottie import st_lottie
# import requests
# 
# # 1) Page config & CSS
# st.set_page_config(
#     page_title="ðŸ’Š MedRec Dashboard",
#     layout="wide",
#     page_icon="ðŸ’Š"
# )
# st.markdown("""
# <div style="background-color:#4B91F6;padding:15px;border-radius:10px">
#   <h1 style="color:white;text-align:center;margin:0">
#     ðŸ’Š Medication Recommendation System
#   </h1>
# </div>
# """, unsafe_allow_html=True)
# 
# # 2) Lottie in sidebar
# def load_lottie(url):
#     r = requests.get(url)
#     return r.json() if r.status_code == 200 else None
# 
# with st.sidebar:
#     lottie_med = load_lottie("https://assets7.lottiefiles.com/packages/lf20_jbrw3hcz.json")
#     if lottie_med:
#         st_lottie(lottie_med, height=200)
# 
# # 3) Load model & data (cached)
# @st.cache_data(show_spinner=False)
# def load_artifacts():
#     model = joblib.load("best_rf_model.joblib")
#     X_test = pd.read_csv("X_test.csv", index_col=0)
#     y_test = pd.read_csv("y_test.csv", index_col=0).values.ravel()
#     explainer = shap.TreeExplainer(model)
#     return model, X_test, y_test, explainer
# 
# with st.spinner("ðŸš€ Loading model & data..."):
#     model, X_test, y_test, explainer = load_artifacts()
#     y_probs = model.predict_proba(X_test)[:, 1]
# 
# # 4) Sidebar controls
# st.sidebar.header("Controls")
# threshold = st.sidebar.slider("Classification threshold", 0.0, 1.0, 0.5, 0.01)
# show_pr = st.sidebar.checkbox("Show PR Curve", True)
# show_cm = st.sidebar.checkbox("Show Confusion Matrix", True)
# 
# # 5) Navigation
# page = st.sidebar.radio("Go to", [
#     "Metrics", "ROC Curve", "Feature Importances",
#     "SHAP: Bar", "SHAP: Beeswarm", "SHAP: Force"
# ])
# 
# # 6) Page: Metrics
# if page == "Metrics":
#     st.header("Model Performance Metrics")
#     metrics = pd.read_csv("model_metrics.csv")
#     st.dataframe(metrics, use_container_width=True)
# 
# # 7) Page: ROC Curve
# elif page == "ROC Curve":
#     st.header("ROC Curve Comparison")
# 
#     st.image(
#         "roc_curve.png",
#         caption="ROC Curves for Random Forest, SVM, and Logistic Regression",
#         use_container_width=False,
#         width=600
#     )
# 
# 
# # 8) Page: Feature Importances
# elif page == "Feature Importances":
#     st.header("Random Forest Feature Importances")
#     feat_imp = pd.read_csv("feature_importances.csv")
#     fig, ax = plt.subplots(figsize=(8,5))
#     sns.barplot(x="Importance", y="Feature", data=feat_imp, ax=ax)
#     ax.set_title("Feature Importances")
#     st.pyplot(fig, use_container_width=True)
# 
# # 9) Page: SHAP: Mean |SHAP| per Feature
# elif page == "SHAP: Bar":
#     st.header("SHAP: Mean |SHAP| per Feature")
# 
#     # 1) Sample your test set
#     Xs = X_test.sample(100, random_state=42)
# 
#     # 2) Compute raw SHAP output
#     raw_vals = explainer.shap_values(Xs)
# 
#     # 3) Unify into a 2-D array (n_samples Ã— n_features)
#     if isinstance(raw_vals, list):
#         # old API: list of arrays per class
#         shap_vals = raw_vals[1]
#     elif isinstance(raw_vals, np.ndarray):
#         shap_vals = raw_vals
#         if shap_vals.ndim > 2:
#             # new API: (samples, features, classes)
#             shap_vals = shap_vals[..., 1]   # pick class-1 slice
#     else:
#         st.error(f"Unexpected SHAP output type: {type(raw_vals)}")
#         shap_vals = np.zeros((len(Xs), Xs.shape[1]))
# 
#     # 4) Verify we now have exactly 2 dimensions
#     assert shap_vals.ndim == 2, f"Expected 2D array, got {shap_vals.shape}"
# 
#     # 5) Compute mean absolute SHAP per feature
#     mean_abs = np.abs(shap_vals).mean(axis=0)
# 
#     # 6) Build a clean DataFrame
#     shap_df = pd.DataFrame({
#         "Feature": Xs.columns.tolist(),
#         "Mean |SHAP|": mean_abs.tolist()
#     }).sort_values("Mean |SHAP|", ascending=False)
# 
#     # 7) (Optional) inspect the numbers
#     st.dataframe(shap_df, use_container_width=True)
# 
#     # 8) Plot a horizontal bar chart
#     fig, ax = plt.subplots(figsize=(8, 5))
#     ax.barh(shap_df["Feature"], shap_df["Mean |SHAP|"])
#     ax.invert_yaxis()
#     ax.set_xlabel("Mean |SHAP|")
#     ax.set_title("SHAP Feature Importances (Class 1)")
#     st.pyplot(fig, use_container_width=True)
# 
# 
# # 10) Page: SHAP Beeswarm
# elif page == "SHAP: Beeswarm":
#     st.header("SHAP Global Summary (Beeswarm)")
# 
#     # Display your static image at a narrower width so the y-axis labels show
#     st.image(
#         "shap_summary_beeswarm.png",
#         caption="SHAP Global Summary (Beeswarm)",
#         use_container_width=False,
#         width=500  # shrink until your feature names are fully visible
#     )
# 
# 
# # 11) Page: SHAP Force
# elif page == "SHAP: Force":
#     st.header("SHAP Force Plot (Sample 0)")
# 
#     # Create three columns so we can center the middle one
#     col1, col2, col3 = st.columns([1, 8, 1])
#     with col2:
#         st.image(
#             "shap_force_plot.png",                # make sure this file lives next to app.py
#             caption="SHAP Force Plot (pre-computed)",
#             use_container_width=True              # fill the column width
#         )
# 
# 
# 
# 
# # 12) Footer in sidebar
# st.sidebar.markdown("---")
# st.sidebar.write("Built with â¤ï¸ â€¢ Powered by Streamlit")
# EOF
# 
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# sed -i \
#   -e 's/st\.experimental_memo/st.cache_data/g' \
#   -e 's/st\.experimental_singleton/st.cache_resource/g' \
#   app.py

# Commented out IPython magic to ensure Python compatibility.
# %%bash --bg
# # start Streamlit and write its logs to streamlit.log
# nohup streamlit run app.py \
#    --server.port 8501 \
#    --server.address 0.0.0.0 \
#    > streamlit.log 2>&1 &
# sleep 2
# echo "â†’ Streamlit log tail:"
# tail -n 20 streamlit.log
#

from google.colab import files
uploaded = files.upload()

!ngrok config add-authtoken 2wBfUzrWR5OUXBz1LkhtvDK32nB_9aUcfPw4pXXQ6nqTjSfA

from pyngrok import ngrok
ngrok.kill()  # Kill all tunnels

# Run Streamlit in background
!nohup streamlit run app.py --server.port 8501 &

#  Wait a few seconds to let it start
import time
time.sleep(5)

# Start ngrok
public_url = ngrok.connect(8501)
print("ðŸ”— Streamlit app is live at:", public_url)

#!streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py