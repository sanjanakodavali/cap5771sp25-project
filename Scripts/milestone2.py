# -*- coding: utf-8 -*-
"""Milestone2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/179q4AUG3F1VyQxPZi6XWwQx84d5REhk_
"""

# For New Data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
import json

from google.colab import drive
drive.mount('/content/drive')

dataset_paths = [
    '/content/drive/MyDrive/Data/medicine_prescription_records.csv',
    '/content/drive/MyDrive/Data/medicine_dataset.csv',
    '/content/drive/MyDrive/Data/medicine_data.csv',
    '/content/drive/MyDrive/Data/drugsComTrain_raw.csv',
    '/content/drive/MyDrive/Data/drug200.csv'
]

def display_csv_heads(paths):
    for path in paths:
        print(f"\n File: {path}")
        try:
            df = pd.read_csv(path)
            print(df.head())
            print(f" Shape: {df.shape} | Columns: {list(df.columns)}\n")
            print("-" * 80)
        except Exception as e:
            print(f" Could not read {path}. Error: {e}\n")

display_csv_heads(dataset_paths)

#  Data Preprocessing

print("Loading datasets...")
medicine_prescription = pd.read_csv('/content/drive/MyDrive/Data/medicine_prescription_records.csv')
medicine_dataset = pd.read_csv('/content/drive/MyDrive/Data/medicine_dataset.csv', low_memory=False)
medicine_data = pd.read_csv('/content/drive/MyDrive/Data/medicine_data.csv')
drugs_reviews = pd.read_csv('/content/drive/MyDrive/Data/drugsComTrain_raw.csv')
drug200 = pd.read_csv('/content/drive/MyDrive/Data/drug200.csv')
print("Datasets loaded successfully.\n")

print("Starting data preprocessing...\n")

# 1. Handling Missing Values
print("Handling missing values...")
medicine_prescription.dropna(inplace=True)
medicine_dataset.fillna("Unknown", inplace=True)
medicine_data.dropna(inplace=True)
drugs_reviews.dropna(subset=['condition', 'review'], inplace=True)
drug200.dropna(inplace=True)
print("Missing values handled.\n")

# 2. Dealing with Mixed Data Types
print("Converting product_price to numeric format...")
medicine_dataset = medicine_dataset.convert_dtypes()
medicine_data['product_price'] = pd.to_numeric(
    medicine_data['product_price'].replace('[\\u20B9,]', '', regex=True), errors='coerce'
)
medicine_data.dropna(subset=['product_price'], inplace=True)
print("Data types converted.\n")

# 3. Parsing drug_interactions JSON column (with enhanced error handling)
print("Parsing drug_interactions column...")
def parse_json_column(x):
    try:
        if isinstance(x, str):
            return list(json.loads(x).get('drug', []))
        return []
    except (json.JSONDecodeError, TypeError, ValueError):
        return []

medicine_data['drug_interactions'] = medicine_data['drug_interactions'].apply(parse_json_column)
print("JSON parsing complete.\n")

# 4. Feature Engineering: Sentiment Analysis on reviews
print("Performing sentiment analysis on reviews...")
drugs_reviews['sentiment_score'] = drugs_reviews['review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)
print("Sentiment scores calculated.\n")

# 5. Data Integration: Merging datasets
print("Merging datasets...")
merged_data = pd.merge(drugs_reviews, medicine_dataset, left_on='drugName', right_on='name', how='left')
merged_data = pd.merge(merged_data, medicine_data, left_on='drugName', right_on='product_name', how='left')
print(f"Merged dataset shape: {merged_data.shape}\n")

# Save preprocessed data
merged_data.to_csv('/content/preprocessed_merged_data.csv', index=False)
print("Preprocessed data saved successfully.\n")

# Exploratory Data Analysis (EDA)

print("Starting Enhanced EDA for Project Objective...\n")

# 1. Summary Statistics
print("Descriptive Statistics for Ratings:\n", merged_data['rating'].describe())
print("Descriptive Statistics for Product Prices:\n", merged_data['product_price'].describe())

# 2. Rating Distribution
plt.figure(figsize=(10, 6))
sns.histplot(merged_data['rating'], bins=10, kde=True, color='skyblue')
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# 3. Sentiment Score Distribution
plt.figure(figsize=(10, 6))
sns.histplot(merged_data['sentiment_score'], bins=20, kde=True, color='lightgreen')
plt.title('Sentiment Score Distribution in Reviews')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

# 4. Top Conditions by Rating
top_conditions = merged_data.groupby('condition')['rating'].mean().sort_values(ascending=False).head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_conditions.index, y=top_conditions.values, palette='viridis')
plt.title('Top 10 Conditions by Average Rating')
plt.ylabel('Average Rating')
plt.xticks(rotation=45)
plt.show()

# 7. Sentiment vs Rating
plt.figure(figsize=(10, 6))
sns.scatterplot(x='sentiment_score', y='rating', data=merged_data, alpha=0.5)
plt.title('Sentiment Score vs. Rating')
plt.xlabel('Sentiment Score')
plt.ylabel('Rating')
plt.show()

# 8. Prescription Trends by Specialty
top_specialties = medicine_prescription['specialty'].value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_specialties.index, y=top_specialties.values, palette='muted')
plt.title('Top 10 Specialties by Prescription Count')
plt.ylabel('Prescriptions')
plt.xticks(rotation=45)
plt.show()

# 9. Outlier Detection for Key Features
fig, axes = plt.subplots(1, 2, figsize=(16, 6))
sns.boxplot(y=merged_data['rating'].dropna(), ax=axes[0], color='coral').set(title='Rating Outliers')
sns.boxplot(y=merged_data['product_price'].dropna(), ax=axes[1], color='lightblue').set(title='Product Price Outliers')
plt.show()

# 10. Drug Interaction Frequency
drug_interactions_flat = [item for sublist in medicine_data['drug_interactions'] for item in sublist]
interaction_counts = pd.Series(drug_interactions_flat).value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=interaction_counts.index, y=interaction_counts.values, palette='Set2')
plt.title('Top 10 Common Drug Interactions')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

# 11. Patient Demographics Analysis
drug200['BP'] = drug200['BP'].astype(str)
sns.countplot(x='BP', data=drug200, palette='pastel')
plt.title('Blood Pressure Distribution in Patients')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(drug200['Na_to_K'], bins=30, kde=True)
plt.title('Distribution of Na_to_K Ratio')
plt.xlabel('Na_to_K Ratio')
plt.ylabel('Frequency')
plt.show()

# --- Milestone 2: Feature Engineering ---

from sklearn.preprocessing import LabelEncoder
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, f1_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler

# Calculate length of each review
merged_data['review_length'] = merged_data['review'].astype(str).apply(len)

# Log transformation to normalize skew in 'usefulCount'
merged_data['log_usefulCount'] = np.log1p(merged_data['usefulCount'])

# Create an interaction term between sentiment and usefulness
merged_data['sentiment_useful_interaction'] = merged_data['sentiment_score'] * merged_data['log_usefulCount']

# Average rating for each condition - might help the model learn better patterns
avg_rating = merged_data.groupby('condition')['rating'].mean().to_dict()
merged_data['avg_rating_per_condition'] = merged_data['condition'].map(avg_rating)

# Encode categorical fields like condition and manufacturer
from sklearn.preprocessing import LabelEncoder

label_enc = LabelEncoder()
merged_data['condition_encoded'] = label_enc.fit_transform(merged_data['condition'].astype(str))
merged_data['manufacturer_encoded'] = label_enc.fit_transform(merged_data['product_manufactured'].astype(str))

# Define binary classification target (1 = good rating, 0 = low rating)
merged_data['good_rating'] = merged_data['rating'].apply(lambda x: 1 if x >= 7 else 0)

# --- Feature Selection ---
# Looking at how features relate to each other

import matplotlib.pyplot as plt
import seaborn as sns

# Select important features for correlation
selected_features = [
    'sentiment_score', 'review_length', 'log_usefulCount',
    'avg_rating_per_condition', 'sentiment_useful_interaction',
    'condition_encoded', 'manufacturer_encoded'
]

# Calculate correlation between features
corr = merged_data[selected_features].corr()

# Plot heatmap with better spacing and visibility
plt.figure(figsize=(14, 10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, square=True,
            cbar_kws={"shrink": 0.75})

plt.xticks(rotation=35, ha='right')
plt.yticks(rotation=0)
plt.title("Correlation Matrix", fontsize=16)
plt.tight_layout(pad=2.0)
plt.show()

# --- Train-Test Split + Modeling ---

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler

# Prepare feature matrix and target
X = merged_data[selected_features].dropna()
y = merged_data.loc[X.index, 'good_rating']

# Split into training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize numeric values for models that need it
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Model Training and Evaluation ---

# Set up models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(probability=True)
}

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

all_metrics = []

# Train and test each model
for name, model in models.items():
    print(f"\n== {name} ==")

    # Train
    if name == 'Random Forest':
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        probs = model.predict_proba(X_test)[:, 1]
    else:
        model.fit(X_train_scaled, y_train)
        preds = model.predict(X_test_scaled)
        probs = model.predict_proba(X_test_scaled)[:, 1]

    # Evaluation
    print(classification_report(y_test, preds))
    auc_score = roc_auc_score(y_test, probs)

    all_metrics.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, preds),
        'Precision': precision_score(y_test, preds),
        'Recall': recall_score(y_test, preds),
        'F1-Score': f1_score(y_test, preds),
        'ROC-AUC': auc_score
    })

    # Plot ROC
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.2f})')

# ROC Curve
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend()
plt.show()

# --- Compare Models in a Table ---

results_df = pd.DataFrame(all_metrics).sort_values(by='F1-Score', ascending=False)
print(results_df)

# Optional: Plot bar chart comparison
results_df.set_index('Model')[['F1-Score', 'ROC-AUC']].plot(kind='bar', figsize=(10, 6), colormap='Set2')
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# --- Compare Models in a Table ---

results_df = pd.DataFrame(all_metrics).sort_values(by='F1-Score', ascending=False)
print(results_df)

# Optional: Plot bar chart comparison
results_df.set_index('Model')[['F1-Score', 'ROC-AUC']].plot(kind='bar', figsize=(10, 6), colormap='Set2')
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()